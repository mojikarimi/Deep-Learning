{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58c910f-1cbe-4962-9b4d-2ee33e741650",
   "metadata": {},
   "source": [
    "![Deep Learning](./Media/DL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50f3f3-b60c-42fd-ba0c-fa36c99416ea",
   "metadata": {},
   "source": [
    "# Deep Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac5710-b1f4-4577-9851-57d95fbf3c42",
   "metadata": {},
   "source": [
    "Deep learning is a subset of machine learning that uses multilayered neural networks, called deep neural networks, to simulate the complex decision-making power of the human brain. Some form of deep learning powers most of the artificial intelligence (AI) applications in our lives today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df166c05-2883-465a-a7a5-134e0fec2e5d",
   "metadata": {},
   "source": [
    "**Deep learning is divided into the following two parts:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b344afb-9a79-4aef-b097-e77fd959d9c1",
   "metadata": {},
   "source": [
    "**1- CV:** Cmputer Vision -> Work with image files(movies).\n",
    "\n",
    "**2- NLP:** Natural Language Processing -> Work with text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5b3c5-70e5-4d66-8dcb-b8e523307c8a",
   "metadata": {},
   "source": [
    "**How does deep learning work?**\n",
    "\n",
    "Deep learning algorithms are neural networks that are modeled after the human brain. For example, a human brain contains millions of interconnected neurons that work together to learn and process information. Similarly, deep learning neural networks, or artificial neural networks, are made of many layers of artificial neurons that work together inside the computer.\n",
    "\n",
    "Artificial neurons are software modules called nodes, which use mathematical calculations to process data. Artificial neural networks are deep learning algorithms that use these nodes to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72438298-2649-43e8-a278-28bf68667870",
   "metadata": {},
   "source": [
    "**Human Brain:**\n",
    "\n",
    "In the human brain, the nerve message is taken to the cell body through the dendrite and is analyzed, then it is transmitted to the next neuron through the axon and the axon terminal.\n",
    "\n",
    "In the human brain, the nerve message is taken to the cell body through the dendrite and is analyzed, then it is transmitted to the next neuron through the axon and the axon terminal.\n",
    "\n",
    "![Neuron](./Media/C-Neuron.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d9979fe-d45f-45da-aaad-d016e0a969e4",
   "metadata": {},
   "source": [
    "**neural network:** When a set of neurons work together, a neural network is formed.\n",
    "\n",
    "![Neural Network](./Media/NW.png)\n",
    "\n",
    "**Hint:** In advanced levels, it is called unit or node `perceptron`.\n",
    "\n",
    "**perceptron:** A mathematical model of a biological neuron.\n",
    "\n",
    "**Our goal in deep learning**: Our goal and work is training perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4311c2-c583-4ecc-baf1-bbb713739279",
   "metadata": {},
   "source": [
    "### Our goal and work is training perceptron:\n",
    "\n",
    "- `Incerdible output`: We go deep into the data and get amazing results\n",
    "- `It moves parallel`: When the amount of data is large, we use deep learning because deep learning works better than machine learning.\n",
    "\n",
    "**GPU:** Graphic Processing Units\n",
    "\n",
    "In deep learning, since calculations and processes are heavy, we need a strong GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f7d1-055a-4cb8-897a-5dd406c9a512",
   "metadata": {},
   "source": [
    "**An example of how to use deep learning:**\n",
    "\n",
    "Deep learning is used in **Netflix**, and after watching a number of movies, it suggests movies that you might like and watch.\n",
    "\n",
    "The most powerful deep learning system is for Netflix and then Amazon.\n",
    "\n",
    "**Artificial intelligence pioneers in the world:**\n",
    "\n",
    "- Google\n",
    "- Microsoft\n",
    "- Tesla\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b2ba1-771f-4325-9f95-fc0c8a7fc51e",
   "metadata": {},
   "source": [
    "## Artificial neural network\n",
    "\n",
    "Artificial Neural Networks contain artificial neurons which are called units. These units are arranged in a series of layers that together constitute the whole Artificial Neural Network in a system. A layer can have only a dozen units or millions of units as this depends on how the complex neural networks will be required to learn the hidden patterns in the dataset. Commonly, Artificial Neural Network has an input layer, an output layer as well as hidden layers. The input layer receives data from the outside world which the neural network needs to analyze or learn about. Then this data passes through one or multiple hidden layers that transform the input into data that is valuable for the output layer. Finally, the output layer provides an output in the form of a response of the Artificial Neural Networks to input data provided.\n",
    "\n",
    "![](./Media/L-DL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e0d45-acec-4dc2-b825-4aaedd475d34",
   "metadata": {},
   "source": [
    "### Layers:\n",
    "\n",
    "- **Input layer:**\n",
    "\n",
    "    Our data is the head of our data set, in other words, our features or our independent variables.\n",
    "\n",
    "- **Hidden layer:**\n",
    "\n",
    "    The part that we don't see is our functions and the information enters this layer from the input layer.\n",
    "\n",
    "    **Hint:** All the units of the input layer must not be connected to all the units of the hidden layer. In other words, we **`feed`** the data from the input to the hidden layer.\n",
    "\n",
    "  There can be several hidden layers, which are called `multiple hidden layers`.<br>\n",
    "  If the hidden layers are the same, it is called `shallow`, and if there are several, they are called `deeper`.\n",
    "\n",
    "- **Output layer:**\n",
    "\n",
    "  This layer corresponds to our output from the algorithm.<br>\n",
    "  This layer is also called `transfer function`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bf171-656b-479e-8c6c-a46def6a6f7f",
   "metadata": {},
   "source": [
    "**Hint:** If all units are supposed to be connected to all units, it is called `fully connected Dense layers` or `fully connected Neural Networks`.\n",
    "\n",
    "**BAT Algorithm:** It creates a frequency. It returns the result that the graph is as below, but we cannot find out where the correct point is because it may continue and the optimal point may change.\n",
    "\n",
    "![BAT Algorithm](./Media/BAT-Algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525b576-7be6-45ce-8daa-ada6ab452f6a",
   "metadata": {},
   "source": [
    "**Propagation:** It is the path we follow in the neural network.\n",
    "\n",
    "_example:_\n",
    "\n",
    "![Propagation](./Media/propa.png)\n",
    "\n",
    "**We have two propagations:**\n",
    "\n",
    "- `Backward Propagation`: input layer -> hidden layers -> output layer\n",
    "- `Forward Propagation`:  output layer -> hidden layers -> input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62ee78-1bc7-42f7-9b65-70ac893edc1e",
   "metadata": {},
   "source": [
    "**W(weight):** `W` tells us how important that data is to us.<br>\n",
    "In the linear regression of machine learning(`mx + c`), the X coefficient is the same W(weight) for us in deep learning.\n",
    "\n",
    "![Propagation](./Media/Weight.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb6dc1-393b-49c1-be55-2f9c08709e9f",
   "metadata": {},
   "source": [
    "**_Example:_**\n",
    "\n",
    "In this example, we have considered a series of data with column headers x one to x three and filled the output value which is yes with yes and no.\n",
    "We determined the weights for that.\n",
    "\n",
    "![Propagation](./Media/Example1.png)\n",
    "\n",
    "\n",
    "When we wrote `mx + c` our goal was simple linear regression where we wanted to find the best value of `m`, but here we want to find the best value of `w`.\n",
    "This process is carried out in such a way that the forward is carried out. If it reaches the value of `yes (correct)`, it changes the value of `W` a little to `optimize` it, but if it reaches the value of `no (false)`, it changes the value of `W` to reach the correct value. `W` is chosen `randomly` for the `first` time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496d716-7852-40c1-9424-ce572fbe8345",
   "metadata": {},
   "source": [
    "### several inputs:\n",
    "\n",
    "If our node takes several inputs, multiple regression is used instead of simple linear regression formula.\n",
    "\n",
    "ML: \n",
    "$$\n",
    "  y = (ax_{1}) + (bx_{2})  + (cx_{3})  ... + (zx_{n}) + inter \n",
    "$$\n",
    "\n",
    "Deep learning:\n",
    "$$\n",
    "  \\Sigma x_{i}w_{i} = (x_{1}w_{1}) + (x_{2}w_{2}) + (x_{3}w_{3}) ... + (x_{n}w_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cf74c-d613-44da-8691-c30d0e40c8f1",
   "metadata": {},
   "source": [
    "**The following formula is used to get the new `w`:**\n",
    "\n",
    "$$\n",
    "W_{new} = \\eta  \\frac {\\partial L}{\\partial w_{old}} + w_{old} \n",
    "$$\n",
    "\n",
    "- **W**: Weight\n",
    "- **L**: Loss\n",
    "  \n",
    "    The amount of our error: $ Loss= (y-\\hat y)^2 $\n",
    "- $ \\partial $: round\n",
    "\n",
    "    The ratio of Y derivative to X derivative: $ \\frac {\\partial Y}{\\partial X} $\n",
    "- $\\eta$: eta\n",
    "  \n",
    "    A fixed number is too small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86010032-8e00-4bbf-8d65-cb2dd3ce08a5",
   "metadata": {},
   "source": [
    "**chain rule chain for Backward:**\n",
    "\n",
    "* We think every time we have a y in every layer.\n",
    "\n",
    "![chain rule](./Media/output.png)\n",
    "\n",
    "**Hint:** Usually, the input layer is not counted except for the number of layers.\n",
    "\n",
    "**If our numbering is as above:**\n",
    "\n",
    "$ Loss= (y-\\hat y)^2 + (y-\\hat y)^2 + (y-\\hat y)^2 $\n",
    "\n",
    "$ Loss= \\Sigma_{i=1} (y_{i} - \\hat y_{i})^2 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ea138-b5e6-4743-b8b3-37c8c8e5c39b",
   "metadata": {},
   "source": [
    "**What is the ratio of loss changes to w11 changes in layer 3?**\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial W_{11}^3} = \\frac{\\partial L}{\\partial O_{31}} \\times \\frac {\\partial O_{31}}{\\partial W_{31}^3} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353c902-54da-416c-aa66-127964b255ea",
   "metadata": {},
   "source": [
    "**Why does one ninety reach the next ninety and one does not?**\n",
    "\n",
    "The human brain comes and says that this neuron does not do any special work, so no message should reach it, it needs to be trained, the human brain needs to be trained so that it does not receive extra information. A node has no use and does nothing, the system comes and deletes it.\n",
    "\n",
    "**Look at the figure below:**\n",
    "\n",
    "![chain rule](./Media/formula1.png)\n",
    "\n",
    "In the figure above, we have a node that has three inputs and one output. In machine learning, if our model was multiple regression, the formula would be as follows:\n",
    "\n",
    "$$\n",
    "  y = (ax_{1}) + (bx_{2})  + (cx_{3}) + d\n",
    "$$\n",
    "\n",
    "In my deep learning, it is almost the same and the formula is as follows:\n",
    "\n",
    "$$\n",
    "  y = (x_{1}w_{1}) + (x_{2}w_{2}) + (x_{3}w_{3}) + bias\n",
    "$$\n",
    "\n",
    "In machine learning, we had interception, but here we have bias, because in addition to calculating the width from the origin, if there is no bias and our input is equal to 0 or a value close to zero, our output is equal to zero and W( if $ x = 0 => w= \\frac{z}{x} => w=\\infty $) is not defined. And bias avoids this problem.\n",
    "\n",
    "Claymon formula in the form of cumin:\n",
    "\n",
    "$$\n",
    "  output = (x_{1}w_{1}) + (x_{2}w_{2}) + (x_{3}w_{3}) ... + (x_{n}w_{n}) + bias\n",
    "$$\n",
    "\n",
    "We call the first part of the formula ($\\Sigma x_{i}w_{i}$) the weighted sum.\n",
    "\n",
    "**Hint:** In deep learning, the goal of the algorithm is to find the best value of `w` and `bias`.\n",
    "\n",
    "**_Example:_**\n",
    "\n",
    "![Example2](./Media/Example2.png)\n",
    "\n",
    "$\n",
    "y = (x_{1}w_{1}) + (x_{2}w_{2}) + bias\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b84d9-86bc-4269-bcb5-8630aa6b88c7",
   "metadata": {},
   "source": [
    "**Matrix multiplication:**\n",
    "\n",
    "In matrix multiplication, each row of the matrix is multiplied by the columns of the other matrix.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\times \\begin{bmatrix} 6 & 3 \\\\ 5 & 2 \\\\ 4 & 1 \\end{bmatrix} = \\begin{bmatrix} 28 & 73 \\\\ 10 & 28 \\end{bmatrix}$$ \n",
    "\n",
    "If we want to write the above (the same example of deep learning) as matrix multiplication, it will be as follows:\n",
    "\n",
    "$y =\\begin{bmatrix} x_{1} & x_{2} \\end{bmatrix} \\times \\begin{bmatrix} w_{1} \\\\ w_{2} \\end{bmatrix} + b$ \n",
    "\n",
    "\n",
    "\n",
    "Now, if we have target and feature, what is the shape of the matrix?\n",
    "\n",
    "N to X <br>\n",
    "N to W\n",
    "\n",
    "$ (1 \\times n) + (n \\times m) + (1 \\times m) = (1 \\times m) $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c2f40-b0a2-4a8e-9a2c-441035a1f35c",
   "metadata": {},
   "source": [
    "**_Example:_**\n",
    "\n",
    "![Example3](./Media/Example3.png)\n",
    "\n",
    "$ \\begin {bmatrix} y_{1} & y_{2} & y_{3} & y_{4} \\end{bmatrix} =\\begin {bmatrix} x_{1} & x_{2} & x_{3} \\end {bmatrix} \\times \\begin{bmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\\\ w_{21} & w_{22} & w_{23} & w_{24} \\\\  w_{31} & w_{32} & w_{33} & w_{34} \\\\ w_{41} & w_{42} & w_{43} & w_{44} \\end{bmatrix} + \\begin{bmatrix} b_{1} & b_{2} & b_{3} & b_{4} \\end{bmatrix} $ \n",
    "\n",
    "$ y_{1} = x_{1}w_{11} + x_{2}w_{21} + x_{3}w_{31} + b_{1} = x_{1}w_{11} + x_{2}w_{21} + b_{1} $<br>\n",
    "$ y_{2} = x_{1}w_{12} + x_{2}w_{22} + x_{3}w_{32} + b_{2} = x_{1}w_{12} + x_{3}w_{31} + b_{2} $<br>\n",
    "$ y_{3} = x_{1}w_{13} + x_{2}w_{23} + x_{3}w_{33} + b_{3} = x_{1}w_{11} + x_{2}w_{21} + b_{3} $<br>\n",
    "$ y_{4} = x_{1}w_{14} + x_{2}w_{24} + x_{3}w_{34} + b_{4} = x_{3}w_{34} + b_{4} $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4baf12-7b7b-4989-87dc-e0da8a4b8cdc",
   "metadata": {},
   "source": [
    "## Activation function:\n",
    "\n",
    "Determines whether the neuron should be activated or not.<br>\n",
    "It means whether the input of the neuron is important or not.<br>\n",
    "The activation function gives a non-linear property to the network because that formula gives us a line and always a linear value, and this is a problem because if not, all the nodes are working the same and the activation function works like a gate and filters the output.\n",
    "\n",
    "**Hint:** Activation functions are one of the most important parameters of deep learning, if they are not there, the algorithm may leave deep learning and become machine learning.\n",
    "\n",
    "![Activation function](./Media/acti1.png)\n",
    "\n",
    "* `Z` = $\\Sigma x_{i}w_{i} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2fe5a-1ccc-4377-9b92-3c39f4afde96",
   "metadata": {},
   "source": [
    "### Differentiation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7e6e0-581c-4d3c-a8c7-0e681434dcc0",
   "metadata": {},
   "source": [
    "- **Power Rule:**\n",
    "\n",
    "  \n",
    "  function: $$ f(x)= x^n $$\n",
    "  derivative:$$ f'(x) nx^{n-1}  $$\n",
    "\n",
    "- **Sum Rule:**\n",
    "\n",
    "  function:$$ f(x) +  g(x) $$\n",
    "  derivative:$$ f'(x) + g'(x)  $$\n",
    "\n",
    "- **Difference Rule:**\n",
    "\n",
    "     function:$$ f(x) -  g(x) $$\n",
    "  derivative:$$ f'(x) - g'(x)  $$\n",
    "\n",
    "\n",
    "- **Product Rule:**\n",
    "\n",
    "  \n",
    "    function:$$ f(x) \\times  g(x) $$\n",
    "  derivative:$$ f'(x)g(x) + g'(x)f(x)  $$\n",
    "\n",
    "- **Quotient Rule:**\n",
    "\n",
    "  function:$$ \\frac {f(x)}{g(x)} $$\n",
    "  derivative:$$ \\frac {f'(x)g(x) - g'(x)f(x)}{g(x)^2}  $$\n",
    "\n",
    "- **Chain Rule:**\n",
    "\n",
    "    function:$$ f(g(x)) $$\n",
    "  derivative:$$ f'(g(x)) \\times g'(x)  $$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e2c5e-edfc-4887-a9a7-8a748eaa2bb1",
   "metadata": {},
   "source": [
    "### Activation of deep learning functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94896e13-f6b8-46b0-b1c3-6323fa14e6f5",
   "metadata": {},
   "source": [
    "- #### sigmoid/logistic\n",
    "\n",
    "    The output of the sigmoid function is like the probability in statistics between zero and one.\n",
    "\n",
    "  **formula:**\n",
    "  $ \\frac{1}{1+e^{-x}} $\n",
    "\n",
    "    **Derivative formula:**\n",
    "    $ \\frac{e^{-x}}{(1+e^{-x})^2} $\n",
    "  \n",
    "   ![sigmoid](./Media/sigmoid_diag.png)\n",
    "\n",
    "  **Difficulties:**\n",
    "  \n",
    "    1. `Efficiency (exponential)`: If our number is too big or too small, calculations will take time and learning the algorithm will take time.\n",
    "    2. First, the slope of the graph decreases and becomes close to zero, and this causes the algorithm to stop because the new W is equal to the old one. This problem is called `gradient vanishing`.\n",
    "  \n",
    "  **positive points:**\n",
    "    1. It normalizes the numbers automatically.\n",
    "    2. It works well for data whose output is binary.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6c6d4-2cdd-4aa3-bf02-da3e356e7965",
   "metadata": {},
   "source": [
    "- **Hyperbolic Tangent:**\n",
    "  \n",
    "    Hyperbolic tangent is a sigmoidal (s-shaped) function like the aforementioned logistic sigmoid function. Where the logistic sigmoid function has outputs between 0 and 1, the hyperbolic tangent has output values between -1 and 1. \n",
    "\n",
    "    This leads to the following advantages over the logistic sigmoid function. The range of [-1, 1] tends to make:\n",
    "\n",
    "    1. negative inputs mapped to strongly negative, zero inputs mapped to near zero, and positive inputs mapped to strongly positive on the tanh graph.\n",
    "    2. each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "\n",
    "    The hyperbolic tangent is a popular activation function that is often used for binary classification and in conjunction with other activation functions that have many nice mathematical properties.\n",
    "\n",
    "    **formula:**\n",
    "  $ \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $\n",
    "\n",
    "    **Derivative formula:**\n",
    "    $ 1 - (\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}})^2 $\n",
    "  \n",
    "   ![Hyperbolic Tangent](./Media/hyper_diag.png)\n",
    "\n",
    "  **Hint:** Like the sigmoid function, this function has the problem of vanishing gradient.\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78d635-4088-408a-b590-e92deee12b1c",
   "metadata": {},
   "source": [
    "- **ReLU (Rectified Linear Unit):**\n",
    "  \n",
    "    ReLU (Rectified Linear Unit) Activation Function is a mathematical function commonly used in artificial neural networks. It is applied to the output of a neuron to introduce non-linearity and enable the network to learn complex patterns and relationships in the data. ReLU function outputs the input value if it is positive, and if the input value is negative, it outputs zero.\n",
    "\n",
    "    **positive points:**\n",
    "  \n",
    "  1. It introduces non-linearity, allowing the network to learn complex patterns and relationships in the data.\n",
    "  2.  It helps in overcoming the vanishing gradient problem, which can hinder the learning process in deep neural networks.\n",
    "  3.  It is computationally efficient compared to other activation functions, such as sigmoid and tanh.\n",
    "\n",
    "    **Difficulties:**\n",
    "\n",
    "    The gradient is zero for all input values ​​that are less than zero, which disables the neurons in that region and may cause the dying ReLU problem.\n",
    "\n",
    "    **formula:**\n",
    "\n",
    "  $\\begin{cases} 0 \\ \\ \\ \\ \\ {x<0} \\\\ {x} \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "    **Derivative formula:**\n",
    "    \n",
    "    $\\begin{cases} 0 \\ \\ \\ \\ \\ {x<0} \\\\ {1} \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "  ![relu](./Media/relu.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd595b-0462-4e4b-998e-89016f3d2917",
   "metadata": {},
   "source": [
    "- **Leaky ReLU:**\n",
    "  \n",
    "  Leaky ReLU function is an improved version of the ReLU activation function. As for the ReLU activation function, the gradient is 0 for all the values of inputs that are less than zero, which would deactivate the neurons in that region and may cause dying ReLU problem.\n",
    "\n",
    "    **formula:**\n",
    "\n",
    "  $\\begin{cases} ax \\ \\ \\ {x<0} \\\\ {x} \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "    **Derivative formula:**\n",
    "    \n",
    "    $\\begin{cases} a \\ \\ \\ \\ \\ {x<0} \\\\ {1} \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "  ![relu](./Media/leaky_diag.png)\n",
    "\n",
    "  **Hint:** Usually, the coefficient of x is a very small value.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade8bb3-6600-4a01-a0a3-0fe1d3cecb37",
   "metadata": {},
   "source": [
    "- **ELU(Exponential Linear Unit):**\n",
    "\n",
    "  The Exponential Linear Unit (ELU) is an activation function for neural networks. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity.\n",
    "\n",
    "  **formula:**\n",
    "\n",
    "  $\\begin{cases} ae^x - 1 \\ \\ \\ \\ \\ {x<0} \\\\ {x} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "    **Derivative formula:**\n",
    "    \n",
    "    $\\begin{cases} ae^x \\ \\ \\ \\ \\ {x<0} \\\\ {1} \\ \\ \\ \\ \\ \\ \\ \\ \\ {x \\ge 0} \\end{cases}$\n",
    "\n",
    "  ![elu](./Media/elu_diag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d8f5e-a601-4812-99cc-60830ebdf68c",
   "metadata": {},
   "source": [
    "- **SoftMax:**\n",
    "\n",
    "    The SoftMax activation function is commonly used in machine learning, particularly in neural networks for classification tasks. An activation function converts a vector of raw prediction scores (logits) into probabilities.\n",
    "\n",
    "\n",
    "\n",
    "  **formula:** $ \\frac {exp(z_{i})}{\\Sigma_{j} exp(z_{j})} $\n",
    "\n",
    "  ![SoftMax](./Media/softmax.png)\n",
    "\n",
    "    **positive points:**\n",
    "  1. `Normalization`: The SoftMax activation function normalizes the input values into a probability distribution, ensuring that the sum of all output values is 1. This makes it suitable for classification problems where the output needs to represent probabilities over multiple classes.\n",
    "    2. `Exponentiation`: By exponentiating the inputs, the SoftMax function in machine learning amplifies the differences between the input values, making the largest value more pronounced in the output probabilities.\n",
    "    3. `Differentiability`: The SoftMax function is differentiable and essential for backpropagation in neural networks.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdec128-f027-464c-870e-a7cbd25621aa",
   "metadata": {},
   "source": [
    "- **Swish:**\n",
    "\n",
    "  The Swish activation function is a mathematical formula that helps a deep learning model make sense of data. It was invented by a group of Google researchers in 2017 and has since shown promising results in many real-world applications.\n",
    "\n",
    "    The Swish function is similar to another function called ReLU, but has a slightly different shape that makes it more efficient and accurate. Unlike ReLU, it also avoids a common problem known as “dying ReLU,” which can cause issues in deep learning models.\n",
    "\n",
    "    The Swish activation function can be used in many different types of machine learning models, including those used for image recognition and speech processing. It’s still being studied to understand all of its capabilities and limitations, but so far, it’s looking very promising!\n",
    "\n",
    "  **formula:**\n",
    "\n",
    "  $ \\frac{x}{1+e^{-ax}} $\n",
    "\n",
    "    **Derivative formula:**\n",
    "    \n",
    "    $ \\frac{axe^{-ax}}{(1+e^{-ax})^2} + \\frac{1}{1+e^{-ax}} $\n",
    "  \n",
    "  ![SoftMax](./Media/swish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544cea2-8a56-4d4d-bdb3-cbab784c667e",
   "metadata": {},
   "source": [
    "- **Soft Plus:**\n",
    "\n",
    "    The Softplus function is a smooth approximation of the ReLU function that removes the knee in the ReLU function graph and replaces it with a smooth curve.\n",
    "  \n",
    "  **formula:**\n",
    "\n",
    "  $ \\log (x + 1) $\n",
    "\n",
    "    **Derivative formula:**\n",
    "    \n",
    "    $ \\frac{e^x}{1+e^{x}} $\n",
    "  \n",
    "  ![SoftMax](./Media/softplus.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
